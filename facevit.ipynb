{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-task Vision Transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import keras\n",
    "from keras import ops\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset values\n",
    "IMAGE_SIZE = 384\n",
    "\n",
    "# Model configs\n",
    "patch_size = 32\n",
    "input_shape = (IMAGE_SIZE, IMAGE_SIZE, 3)  # input image shape\n",
    "learning_rate = 0.001\n",
    "weight_decay = 0.0001\n",
    "batch_size = 256\n",
    "num_epochs = 100\n",
    "num_patches = (IMAGE_SIZE // patch_size) ** 2\n",
    "projection_dim = 64\n",
    "num_heads = 4\n",
    "age_k = 3\n",
    "\n",
    "# Transformer layers\n",
    "transformer_units = [\n",
    "    projection_dim * 2,\n",
    "    projection_dim,\n",
    "]\n",
    "transformer_layers = 4\n",
    "\n",
    "# MLP units\n",
    "mlp_head_units = [2048, 1024, 512, 64, 32]\n",
    "\n",
    "\n",
    "# FaceVit losses and metrics\n",
    "facevit_losses = {\n",
    "    \"face\": keras.losses.MeanSquaredError(),\n",
    "    \"gender\": keras.losses.BinaryCrossentropy(),\n",
    "    \"age\": keras.losses.CategoricalFocalCrossentropy()\n",
    "}\n",
    "\n",
    "facevit_metrics = {\n",
    "    \"face\": 'mse',\n",
    "    \"gender\": 'accuracy',\n",
    "    \"age\": tf.keras.metrics.TopKCategoricalAccuracy(k=age_k, name=f'top_{age_k}_accuracy', dtype=None)\n",
    "}\n",
    "\n",
    "def model_compiler(model, optimizer, loss, metrics):\n",
    "    \"Model compilation function\"\n",
    "    model.compile(optimizer= optimizer, loss= loss, metrics = metrics)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and prepare dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_age_bins_and_encode(df,\n",
    "                               bin_size=5,\n",
    "                               max_age = 100,\n",
    "                               json_output_path='/content/drive/MyDrive/FaceViT/age_bins.json',\n",
    "                               write_json = True):\n",
    "    \"\"\"\n",
    "    Function to bin ages into configurable intervals, encode the bins, and save the encoding dictionary in JSON format.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): DataFrame containing the age column.\n",
    "    bin_size (int): Size of each age bin (default is 5 years).\n",
    "    json_output_path (str): Path to save the JSON dictionary mapping age bins to encoded labels.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame with an additional column for encoded age bins.\n",
    "    \"\"\"\n",
    "    import json\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "    # Define the age bins and bin labels\n",
    "    bins = range(0, max_age, bin_size)\n",
    "    labels = [f'{i}-{i + bin_size - 1}' for i in range(0, max_age- bin_size, bin_size)]\n",
    "\n",
    "    # Bin the ages\n",
    "    df['age_bin'] = pd.cut(df['age'], bins=bins, labels=labels, right=False)\n",
    "\n",
    "    # Encode the age bins into class indexes\n",
    "    label_encoder = LabelEncoder()\n",
    "    df['age_class'] = label_encoder.fit_transform(df['age_bin'])\n",
    "    age_bin_mapping = dict(zip(df['age_bin'], df['age_class']))\n",
    "\n",
    "    # Save the dictionary as a JSON file\n",
    "    if write_json:\n",
    "        with open(json_output_path, 'w') as json_file:\n",
    "            json.dump(age_bin_mapping, json_file)\n",
    "\n",
    "    return df, len(age_bin_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmentations pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_color_augmentation(image):\n",
    "    augmentations = [\n",
    "        lambda img: tf.image.random_brightness(img, max_delta=0.2),\n",
    "        lambda img: tf.image.random_contrast(img, lower=0.5, upper=1.5),\n",
    "        lambda img: tf.image.random_saturation(img, lower=0.5, upper=1.5),\n",
    "        lambda img: tf.image.random_hue(img, max_delta=0.2)\n",
    "    ]\n",
    "\n",
    "    # Randomly select 2 augmentations to apply\n",
    "    chosen_augmentations = random.sample(augmentations, 2)\n",
    "\n",
    "    for aug in chosen_augmentations:\n",
    "        image = aug(image)\n",
    "\n",
    "    # Ensure values remain within 0 and 1\n",
    "    image = tf.clip_by_value(image, 0.0, 1.0)\n",
    "\n",
    "    return image\n",
    "\n",
    "def apply_augmentations_fn(images, ground_truth):\n",
    "    \"\"\"Only color based augmentatios are used for simplicity\"\"\"\n",
    "    # Apply some simple color augmentations to the images\n",
    "    augmented_images = tf.map_fn(apply_color_augmentation, images)\n",
    "    return augmented_images, ground_truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tf_dataset(csv_file, images_dir, batch_size, target_size=(384, 384), augment=False, write_json = True):\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(csv_file)\n",
    "\n",
    "    # Convert gender to 0 for male and 1 for female\n",
    "    df['gender'] = df['gender'].map({'M': 0, 'F': 1})\n",
    "\n",
    "    # Extract relevant columns (age, gender, and face bounding box)\n",
    "    df = df[['img_name','age', 'gender', 'face_x0', 'face_y0', 'face_x1', 'face_y1']]\n",
    "\n",
    "    # Create file paths for images\n",
    "    df['img_path'] = images_dir + '/' + df['img_name']\n",
    "\n",
    "    # Convert box to numeric\n",
    "    df['face_x0'] = pd.to_numeric(df['face_x0'])\n",
    "    df['face_y0'] = pd.to_numeric(df['face_y0'])\n",
    "    df['face_x1'] = pd.to_numeric(df['face_x1'])\n",
    "    df['face_y1'] = pd.to_numeric(df['face_y1'])\n",
    "\n",
    "    # Create the age bin with bins per 5 years for classification task\n",
    "    # Save the labels dictionary to a json file for decoding predictions\n",
    "    df, num_age_groups = create_age_bins_and_encode(df, bin_size=5, write_json = write_json)\n",
    "\n",
    "    def load_and_preprocess_image(img_path, age, gender, face_x0, face_y0, face_x1, face_y1, age_groups_n = num_age_groups):\n",
    "        # Load image\n",
    "        img = tf.io.read_file(img_path)\n",
    "        img = tf.image.decode_jpeg(img, channels=3)\n",
    "\n",
    "        # Get original width and height before resizing to scale the boxes\n",
    "        original_height = tf.cast(tf.shape(img)[0], dtype=tf.int64)\n",
    "        original_width = tf.cast(tf.shape(img)[1], dtype=tf.int64)\n",
    "\n",
    "        # Rescale/normalize bounding box coordinates\n",
    "        face_x0_scaled = face_x0 / original_width\n",
    "        face_y0_scaled = face_y0 / original_height\n",
    "        face_x1_scaled = face_x1 / original_width\n",
    "        face_y1_scaled = face_y1 / original_height\n",
    "\n",
    "        # Resize and normalize\n",
    "        img = tf.image.resize(img, target_size)\n",
    "        img = tf.cast(img, tf.float32) / 255.0\n",
    "\n",
    "        # Concatenate normalized coordinates\n",
    "        bounding_box = tf.convert_to_tensor([face_x0_scaled, face_y0_scaled,\n",
    "                                            face_x1_scaled, face_y1_scaled])\n",
    "\n",
    "        # Convert gender and age to tensors\n",
    "        gender_tensor = tf.cast(tf.convert_to_tensor(gender), dtype=tf.int32)\n",
    "        age_tensor = tf.cast(tf.convert_to_tensor(age), dtype=tf.int32)\n",
    "        age_tensor = tf.one_hot(age_tensor, depth=age_groups_n)\n",
    "\n",
    "        # Return image and concatenated ground truth tensor\n",
    "        return img, (bounding_box, gender_tensor, age_tensor)\n",
    "\n",
    "\n",
    "    # Create TensorFlow dataset from DataFrame\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((\n",
    "        df['img_path'].values,\n",
    "        df['age_class'].values,\n",
    "        df['gender'].values,\n",
    "        df['face_x0'].values,\n",
    "        df['face_y0'].values,\n",
    "        df['face_x1'].values,\n",
    "        df['face_y1'].values\n",
    "    ))\n",
    "\n",
    "    # Load and preprocess images in parallel\n",
    "    dataset = dataset.map(load_and_preprocess_image, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    # Batch the dataset\n",
    "    dataset = dataset.batch(batch_size, drop_remainder= True)\n",
    "\n",
    "    if augment:\n",
    "        dataset = dataset.map(apply_augmentations_fn, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    return dataset, num_age_groups\n",
    "\n",
    "def plot_image_with_boxes(image, bbox, image_size = (384,384,3)):\n",
    "    \"\"\"Helper plotting function\"\"\"\n",
    "    # Create figure and axis\n",
    "    fig, ax = plt.subplots(1)\n",
    "\n",
    "    # Display the image\n",
    "    ax.imshow(image)\n",
    "    h, w, _ = image_size\n",
    "\n",
    "    bbox = [coord for coord in bbox]\n",
    "\n",
    "    x1, y1, x2, y2 = bbox\n",
    "\n",
    "    # Scale and calculate width and height\n",
    "    x1 = x1 * w\n",
    "    y1 = y1 * h\n",
    "    x2 = x2 * w\n",
    "    y2 = y2 * h\n",
    "    width = x2 - x1\n",
    "    height = y2 - y1\n",
    "\n",
    "    # Create a rectangle patch\n",
    "    rect = patches.Rectangle((x1, y1), width, height, linewidth=1, edgecolor='r', facecolor='none')\n",
    "    # Add the patch to the plot\n",
    "    ax.add_patch(rect)\n",
    "\n",
    "    # Show plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths and parameters\n",
    "train_csv = 'utk_faces/train.csv'\n",
    "images_dir = 'utk_faces/utk_train/train'\n",
    "\n",
    "# Create the dataset\n",
    "train_dataset, num_age_groups = create_tf_dataset(train_csv, images_dir, batch_size, augment= True)\n",
    "\n",
    "# Example usage (iterating over the dataset)\n",
    "for batch in train_dataset.take(1):\n",
    "    images, y_true = batch\n",
    "    bounding_boxes, genders, ages = y_true\n",
    "\n",
    "    # Extract the first image and its corresponding bounding boxes\n",
    "    for i in range(0,5):\n",
    "        img = images[i].numpy()\n",
    "        box = bounding_boxes[i].numpy()\n",
    "\n",
    "        # Plot the first image with bounding boxes\n",
    "        plot_image_with_boxes(img, box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths and parameters\n",
    "val_csv = 'utk_faces/val.csv'\n",
    "images_dir = 'utk_faces/utk_val/val'\n",
    "\n",
    "# Create the dataset\n",
    "val_dataset, _ = create_tf_dataset(val_csv, images_dir, batch_size, write_json = False)\n",
    "\n",
    "# Example usage (iterating over the dataset)\n",
    "for batch in val_dataset.take(1):\n",
    "    images, y_true = batch\n",
    "    bounding_boxes, genders, ages = y_true\n",
    "\n",
    "    # Extract the first image and its corresponding bounding boxes\n",
    "    first_image = images[0].numpy()\n",
    "    first_bounding_boxes = bounding_boxes[0].numpy()\n",
    "\n",
    "    # Plot the first image with bounding boxes\n",
    "    plot_image_with_boxes(first_image, first_bounding_boxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FaceVit model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(x, hidden_units, dropout_rate, block_name):\n",
    "    \"\"\"Simple MLP with dropout\"\"\"\n",
    "    for i in range(len(hidden_units)):\n",
    "        x = layers.Dense(hidden_units[i], activation=keras.activations.gelu, name= f'Dense_{i}_{block_name}')(x)\n",
    "        x = layers.Dropout(dropout_rate, name = f'Dropout_{i}_{block_name}')(x)\n",
    "    return x\n",
    "\n",
    "class Patches(layers.Layer):\n",
    "    def __init__(self, patch_size, name):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.name = name\n",
    "\n",
    "    def call(self, images):\n",
    "        input_shape = ops.shape(images)\n",
    "        batch_size = input_shape[0]\n",
    "        height = input_shape[1]\n",
    "        width = input_shape[2]\n",
    "        channels = input_shape[3]\n",
    "        num_patches_h = height // self.patch_size\n",
    "        num_patches_w = width // self.patch_size\n",
    "        patches = keras.ops.image.extract_patches(images, size=self.patch_size)\n",
    "        patches = ops.reshape(\n",
    "            patches,\n",
    "            (\n",
    "                batch_size,\n",
    "                num_patches_h * num_patches_w,\n",
    "                self.patch_size * self.patch_size * channels,\n",
    "            ),\n",
    "        )\n",
    "        return patches\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\"patch_size\": self.patch_size,\n",
    "                       'name': self.name})\n",
    "        return config\n",
    "\n",
    "class PatchEncoder(layers.Layer):\n",
    "    def __init__(self, num_patches, projection_dim, name):\n",
    "        super().__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.projection = layers.Dense(units=projection_dim)\n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=num_patches, output_dim=projection_dim\n",
    "        )\n",
    "        self.name = name\n",
    "\n",
    "    # Override function to avoid error while saving model\n",
    "    def get_config(self):\n",
    "        config = super().get_config().copy()\n",
    "        config.update(\n",
    "            {   \"name\": self.name,\n",
    "                \"input_shape\": input_shape,\n",
    "                \"patch_size\": patch_size,\n",
    "                \"num_patches\": num_patches,\n",
    "                \"projection_dim\": projection_dim,\n",
    "                \"num_heads\": num_heads,\n",
    "                \"transformer_units\": transformer_units,\n",
    "                \"transformer_layers\": transformer_layers,\n",
    "                \"mlp_head_units\": mlp_head_units,\n",
    "            }\n",
    "        )\n",
    "        return config\n",
    "\n",
    "    def call(self, patch):\n",
    "        positions = ops.expand_dims(\n",
    "            ops.arange(start=0, stop=self.num_patches, step=1), axis=0\n",
    "        )\n",
    "        projected_patches = self.projection(patch)\n",
    "        encoded = projected_patches + self.position_embedding(positions)\n",
    "        return encoded\n",
    "    \n",
    "class Class_Embeddings(layers.Layer):\n",
    "    def __init__(self, projection_dim, name=None):\n",
    "        super(Class_Embeddings, self).__init__(name=name)\n",
    "        self.projection_dim = projection_dim\n",
    "        self.age_cls_embedding = self.add_weight(\n",
    "            shape=(1, 1, projection_dim),\n",
    "            initializer='random_normal',\n",
    "            trainable=True,\n",
    "            name='age_cls_embedding'\n",
    "        )\n",
    "        self.gender_cls_embedding = self.add_weight(\n",
    "            shape=(1, 1, projection_dim),\n",
    "            initializer='random_normal',\n",
    "            trainable=True,\n",
    "            name='gender_cls_embedding'\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        age_cls_embedding = tf.tile(self.age_cls_embedding, [batch_size, 1, 1])\n",
    "        gender_cls_embedding = tf.tile(self.gender_cls_embedding, [batch_size, 1, 1])\n",
    "        return age_cls_embedding, gender_cls_embedding\n",
    "    \n",
    "def build_facevit(\n",
    "    input_shape,\n",
    "    patch_size,\n",
    "    num_patches,\n",
    "    projection_dim,\n",
    "    num_heads,\n",
    "    transformer_units,\n",
    "    transformer_layers,\n",
    "    mlp_head_units,\n",
    "    age_bins_num\n",
    "):\n",
    "    inputs = keras.Input(shape=input_shape, name = 'Input')\n",
    "\n",
    "    # Create patches\n",
    "    patches = Patches(patch_size, name = 'Patch_creator')(inputs)\n",
    "    # Encode patches\n",
    "    encoded_patches = PatchEncoder(num_patches, projection_dim, name ='Patch_encoder')(patches)\n",
    "\n",
    "    # Create the class tokens for the classification tasks\n",
    "    class_tokens = Class_Embeddings(projection_dim, name='Class_Encoder')\n",
    "    age_cls_embedding, gender_cls_embedding = class_tokens(inputs)\n",
    "\n",
    "    # Pre-pend the tokens to the encoded_patches (age then gender)\n",
    "    encoded_patches = layers.Concatenate(axis=1, name= 'embed_concat')([age_cls_embedding, gender_cls_embedding, encoded_patches])\n",
    "\n",
    "    # Create multiple layers of the Transformer block.\n",
    "    for i in range(transformer_layers):\n",
    "        # Layer normalization 1.\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-6, name = f'LayerNorm_1_block_{i}')(encoded_patches)\n",
    "        # Create a multi-head attention layer.\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1, name = f'MultiHeadAttn_block_{i}'\n",
    "        )(x1, x1)\n",
    "        # Skip connection 1.\n",
    "        x2 = layers.Add(name = f'Skip_1_block_{i}')([attention_output, encoded_patches])\n",
    "        # Layer normalization 2.\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-6, name = f'LayerNorm_2_block_{i}')(x2)\n",
    "        # MLP\n",
    "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1, block_name = f'trans_block_{i}')\n",
    "        # Skip connection 2.\n",
    "        encoded_patches = layers.Add(name = f'Skip_2_block_{i}')([x3, x2])\n",
    "\n",
    "    # Create a [batch_size, projection_dim] tensor.\n",
    "    representation = layers.LayerNormalization(epsilon=1e-6, name ='LayerNorm_transformed')(encoded_patches)\n",
    "    representation = layers.Flatten(name = 'Flatten_transformed')(representation[:, 2:, :])\n",
    "    representation = layers.Dropout(0.3, name = 'Dropout_transformed')(representation)\n",
    "\n",
    "    # Get the transformed class/age tokens for the classifation tasks\n",
    "    age_token = encoded_patches[:, 0, :]   \n",
    "    gender_token = encoded_patches[:, 1, :]\n",
    "\n",
    "    # Add MLP.\n",
    "    features_box = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.3, block_name= 'MLP_box_out')\n",
    "    features_age = mlp(age_token, hidden_units=mlp_head_units, dropout_rate=0.3, block_name= 'MLP_age_out')\n",
    "    features_gender = mlp(gender_token, hidden_units=mlp_head_units, dropout_rate=0.3, block_name= 'MLP_gender_out')\n",
    "\n",
    "    # FaceViT output layers\n",
    "    face_box = layers.Dense(4, activation = 'sigmoid', name=\"face\")(features_box)\n",
    "    gender_classifier = layers.Dense(1, activation='sigmoid', name = 'gender')(features_gender)\n",
    "    age_classifier = layers.Dense(age_bins_num, activation= 'softmax', name= 'age') (features_age)\n",
    "\n",
    "    return keras.Model(inputs=inputs, outputs=[face_box, gender_classifier, age_classifier], name = 'FaceVit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_facevit(input_shape,\n",
    "                      patch_size,\n",
    "                      num_patches,\n",
    "                      projection_dim,\n",
    "                      num_heads,\n",
    "                      transformer_units,\n",
    "                      transformer_layers,\n",
    "                      mlp_head_units,\n",
    "                      num_age_groups\n",
    "                    )\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test patches layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_dataset))\n",
    "images, _ = batch\n",
    "image = images[0] * 255\n",
    "image = image.numpy()\n",
    "\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.imshow(image.astype(\"uint8\"))\n",
    "plt.axis(\"off\")\n",
    "\n",
    "patches = Patches(patch_size, name= 'Patches')(np.expand_dims(image, axis=0))\n",
    "print(f\"Image size: {IMAGE_SIZE} X {IMAGE_SIZE}\")\n",
    "print(f\"Patch size: {patch_size} X {patch_size}\")\n",
    "print(f\"{patches.shape[1]} patches per image \\n{patches.shape[-1]} elements per patch\")\n",
    "\n",
    "\n",
    "n = int(np.sqrt(patches.shape[1]))\n",
    "plt.figure(figsize=(4, 4))\n",
    "for i, patch in enumerate(patches[0]):\n",
    "    ax = plt.subplot(n, n, i + 1)\n",
    "    patch_img = ops.reshape(patch, (patch_size, patch_size, 3))\n",
    "    plt.imshow(ops.convert_to_numpy(patch_img).astype(\"uint8\"))\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(model, \n",
    "                   train_ds, \n",
    "                   validation_ds, \n",
    "                   learning_rate, \n",
    "                   weight_decay,\n",
    "                   batch_size, \n",
    "                   num_epochs,\n",
    "                   loss_func,\n",
    "                   metrics):\n",
    "    optimizer = keras.optimizers.AdamW(\n",
    "        learning_rate=learning_rate, weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "    # Compile model.\n",
    "    model = model_compiler(model, optimizer, loss= loss_func, metrics= metrics)\n",
    "\n",
    "    checkpoint_filepath = \"./checkpoints/facevit.weights.h5\"\n",
    "    checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "        checkpoint_filepath,\n",
    "        monitor=\"val_loss\",\n",
    "        save_best_only=True,\n",
    "        save_weights_only=True,\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        batch_size=batch_size,\n",
    "        epochs=num_epochs,\n",
    "        validation_data=validation_ds,\n",
    "        callbacks=[\n",
    "            checkpoint_callback,\n",
    "            keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=10),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return history\n",
    "\n",
    "# Train model\n",
    "history = run_experiment(\n",
    "    model,\n",
    "    train_dataset.take(1),\n",
    "    val_dataset.take(1),\n",
    "    learning_rate, \n",
    "    weight_decay, \n",
    "    batch_size, \n",
    "    num_epochs,\n",
    "    facevit_losses,\n",
    "    facevit_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(item):\n",
    "    plt.plot(history.history[item], label=item)\n",
    "    plt.plot(history.history[\"val_\" + item], label=\"val_\" + item)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(item)\n",
    "    plt.title(\"Train and Validation {} Over Epochs\".format(item), fontsize=14)\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "plot_history(\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(history):\n",
    "    history = history.history\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "    # Plot age accuracy\n",
    "    axes[0].plot(history[f'age_top_{age_k}_accuracy'], label='Train Accuracy', marker='o')\n",
    "    axes[0].plot(history[f'val_age_top_{age_k}_accuracy'], label='Validation Accuracy', marker='o')\n",
    "    axes[0].set_title(f'Top-{age_k} age Accuracy')\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Accuracy')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True)\n",
    "\n",
    "    # Plot face mse\n",
    "    axes[1].plot(history['face_mse'], label='Train MSE', marker='o')\n",
    "    axes[1].plot(history['val_face_mse'], label='Validation MSE', marker='o')\n",
    "    axes[1].set_title('Face box MSE')\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('MSE')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True)\n",
    "\n",
    "    # Plot gender accuracy\n",
    "    axes[2].plot(history['gender_accuracy'], label='Train Accuracy', marker='o')\n",
    "    axes[2].plot(history['val_gender_accuracy'], label='Validation Accuracy', marker='o')\n",
    "    axes[2].set_title('Gender Accuracy')\n",
    "    axes[2].set_xlabel('Epoch')\n",
    "    axes[2].set_ylabel('Accuracy')\n",
    "    axes[2].legend()\n",
    "    axes[2].grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_metrics(history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
